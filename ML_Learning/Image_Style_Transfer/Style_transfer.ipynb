{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style_transfer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOQDf8mdxFfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing import image as kp_image\n",
        "\n",
        "# Keras is only used to load VGG19 model as a high level API to TensorFlow\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "# pillow is used for loading and saving images\n",
        "from PIL import Image\n",
        "\n",
        "# numPy is used for manipulation of array of object i.e Image in our case\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHq_UQupxOB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of layers to be considered for calculation of Content and Style Loss\n",
        "content_layers = ['block3_conv3']\n",
        "style_layers   = ['block1_conv1','block2_conv2','block4_conv3']\n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers   = len(style_layers)\n",
        "\n",
        "# path where the content and style images are located\n",
        "content_path = 'content.jpg'\n",
        "style_path   = 'style.jpg'\n",
        "\n",
        "# Save the result as\n",
        "save_name = 'generated.jpg'\n",
        "\n",
        "# path to where Vgg19 model weight is located\n",
        "vgg_weights = \"vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tkL1XqRxbL1",
        "colab_type": "text"
      },
      "source": [
        "UTILS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec6IOJj6xfIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_img(path_to_img):\n",
        "\n",
        "  max_dim  = 512\n",
        "  img      = Image.open(path_to_img)\n",
        "  img_size = max(img.size)\n",
        "  scale    = max_dim/img_size\n",
        "  img      = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n",
        "\n",
        "  img      = kp_image.img_to_array(img)\n",
        "\n",
        "  # We need to broadcast the image array such that it has a batch dimension\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "\n",
        "  # preprocess raw images to make it suitable to be used by VGG19 model\n",
        "  out = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "\n",
        "  return tf.convert_to_tensor(out)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcPVSBp5xxNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deprocess_img(processed_img):\n",
        "    x = processed_img.copy()\n",
        "\n",
        "    # perform the inverse of the preprocessiing step\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "\n",
        "    x = x[:, :, ::-1]\n",
        "\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfmFQESVx0Nz",
        "colab_type": "text"
      },
      "source": [
        "Loss Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oz5FHT4x3lQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Content Loss Function\n",
        "### Content Loss Function\n",
        "def get_content_loss(content, target):\n",
        "  return tf.reduce_mean(tf.square(content - target)) /2\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wW_-bXqx9T6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Style Loss Fucntion\n",
        "def gram_matrix(input_tensor):\n",
        "    # if input tensor is a 3D array of size Nh x Nw X Nc\n",
        "    # we reshape it to a 2D array of Nc x (Nh*Nw)\n",
        "    channels = int(input_tensor.shape[-1])\n",
        "    a = tf.reshape(input_tensor, [-1, channels])\n",
        "    n = tf.shape(a)[0]\n",
        "\n",
        "    # get gram matrix\n",
        "    gram = tf.matmul(a, a, transpose_a=True)\n",
        "\n",
        "    return gram\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdbIekX1yAzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_style_loss(base_style, gram_target):\n",
        "    height, width, channels = base_style.get_shape().as_list()\n",
        "    gram_style = gram_matrix(base_style)\n",
        "\n",
        "    # Original eqn as a constant to divide i.e 1/(4. * (channels ** 2) * (width * height) ** 2)\n",
        "    return tf.reduce_mean(tf.square(gram_style - gram_target)) / (\n",
        "                channels ** 2 * width * height)  # (4.0 * (channels ** 2) * (width * height) ** 2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDK7HQUWyDq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Use to pass content and style image through it\n",
        "def get_feature_representations(model, content_path, style_path, num_content_layers):\n",
        "    # Load our images in\n",
        "    content_image = load_img(content_path)\n",
        "    style_image = load_img(style_path)\n",
        "\n",
        "    # batch compute content and style features\n",
        "    content_outputs = model(content_image)\n",
        "    style_outputs = model(style_image)\n",
        "\n",
        "    # Get the style and content feature representations from our model\n",
        "    style_features = [style_layer[0] for style_layer in style_outputs[num_content_layers:]]\n",
        "    content_features = [content_layer[0] for content_layer in content_outputs[:num_content_layers]]\n",
        "\n",
        "    return style_features, content_features\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHi4sGQ_yMMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Total Loss\n",
        "def compute_loss(model, loss_weights, generated_output_activations, gram_style_features, content_features,\n",
        "                 num_content_layers, num_style_layers):\n",
        "    generated_content_activations = generated_output_activations[:num_content_layers]\n",
        "    generated_style_activations = generated_output_activations[num_content_layers:]\n",
        "\n",
        "    style_weight, content_weight = loss_weights\n",
        "\n",
        "    style_score = 0\n",
        "    content_score = 0\n",
        "\n",
        "    # Accumulate style losses from all layers\n",
        "    # Here, we equally weight each contribution of each loss layer\n",
        "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
        "    for target_style, comb_style in zip(gram_style_features, generated_style_activations):\n",
        "        temp = get_style_loss(comb_style[0], target_style)\n",
        "        style_score += weight_per_style_layer * temp\n",
        "\n",
        "    # Accumulate content losses from all layers\n",
        "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
        "    for target_content, comb_content in zip(content_features, generated_content_activations):\n",
        "        temp = get_content_loss(comb_content[0], target_content)\n",
        "        content_score += weight_per_content_layer * temp\n",
        "\n",
        "    # Get total loss\n",
        "    loss = style_weight * style_score + content_weight * content_score\n",
        "\n",
        "    return loss, style_score, content_score\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BCOlJ1RyOl4",
        "colab_type": "text"
      },
      "source": [
        "CREATE STYLE TRANSFER\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZJFgfOnyTdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Keras Load VGG19 model\n",
        "def get_model(content_layers, style_layers):\n",
        "    # Load our model. We load pretrained VGG, trained on imagenet data\n",
        "    vgg19 = VGG19(weights=None, include_top=False)\n",
        "\n",
        "    # We don't need to (or want to) train any layers of our pre-trained vgg model, so we set it's trainable to false.\n",
        "    vgg19.trainable = False\n",
        "\n",
        "    style_model_outputs = [vgg19.get_layer(name).output for name in style_layers]\n",
        "    content_model_outputs = [vgg19.get_layer(name).output for name in content_layers]\n",
        "\n",
        "    model_outputs = content_model_outputs + style_model_outputs\n",
        "\n",
        "    # Build model\n",
        "    return Model(inputs=vgg19.input, outputs=model_outputs), vgg19\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cVaPvQ6yhxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def run_style_transfer(content_path, style_path, num_iterations=200, content_weight=0.1, style_weight=0.9):\n",
        "    # Create a tensorflow session\n",
        "    sess = tf.Session()\n",
        "\n",
        "    # Assign keras back-end to the TF session which we created\n",
        "    K.set_session(sess)\n",
        "\n",
        "    model, vgg19 = get_model(content_layers, style_layers)\n",
        "\n",
        "    # Get the style and content feature representations (from our specified intermediate layers)\n",
        "    style_features, content_features = get_feature_representations(model, content_path, style_path, num_content_layers)\n",
        "    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "\n",
        "    # VGG default normalization\n",
        "    norm_means = np.array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means\n",
        "\n",
        "    # In original paper, the initial stylized image is random matrix of same size as that of content image\n",
        "    # but in later images content image was used instead on random values for first stylized image\n",
        "    # because it proved to help to stylize faster\n",
        "    generated_image = load_img(content_path)\n",
        "    # generated_image = np.random.randint(0,255, size=generated_image.shape)\n",
        "\n",
        "    # Create tensorflow variable to hold a stylized/generated image during the training\n",
        "    generated_image = tf.Variable(generated_image, dtype=tf.float32)\n",
        "\n",
        "    model_outputs = model(generated_image)\n",
        "\n",
        "    # weightages of each content and style images i.e alpha & beta\n",
        "    loss_weights = (style_weight, content_weight)\n",
        "\n",
        "    # Create our optimizer\n",
        "    loss = compute_loss(model, loss_weights, model_outputs, gram_style_features, content_features, num_content_layers,\n",
        "                        num_style_layers)\n",
        "    opt = tf.train.AdamOptimizer(learning_rate=9, beta1=0.9, epsilon=1e-1).minimize(loss[0], var_list=[generated_image])\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(generated_image.initializer)\n",
        "\n",
        "    # loading the weights again because tf.global_variables_initializer() resets the weights\n",
        "    vgg19.load_weights(vgg_weights)\n",
        "\n",
        "    # Put loss as infinity before training starts and Create a variable to hold best image (i.e image with minimum loss)\n",
        "    best_loss, best_img = float('inf'), None\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        # Do optimization\n",
        "        sess.run(opt)\n",
        "\n",
        "        # Make sure image values stays in the range of max-min value of VGG norm\n",
        "        clipped = tf.clip_by_value(generated_image, min_vals, max_vals)\n",
        "        # assign the clipped value to the tensor stylized image\n",
        "        generated_image.assign(clipped)\n",
        "\n",
        "        # Open the Tuple of tensors\n",
        "        total_loss, style_score, content_score = loss\n",
        "        total_loss = total_loss.eval(session=sess)\n",
        "\n",
        "        if total_loss < best_loss:\n",
        "            # Update best loss and best image from total loss.\n",
        "            best_loss = total_loss\n",
        "\n",
        "            # generated image is of shape (1, h, w, 3) convert it to (h, w, 3)\n",
        "            temp_generated_image = sess.run(generated_image)[0]\n",
        "            best_img = deprocess_img(temp_generated_image)\n",
        "\n",
        "            s_loss = sess.run(style_score)\n",
        "            c_loss = sess.run(content_score)\n",
        "\n",
        "            # print best loss\n",
        "            print('best: iteration: ', i, 'loss: ', total_loss, '  style_loss: ', s_loss, '  content_loss: ', c_loss)\n",
        "\n",
        "        # Save image after every 100 iterations\n",
        "        if (i + 1) % 50 == 0:\n",
        "            output = Image.fromarray(best_img)\n",
        "            output.save(str(i + 1) + '-' + save_name)\n",
        "\n",
        "    # after num_iterations iterations are completed, close the TF session\n",
        "    sess.close()\n",
        "\n",
        "    return best_img, best_loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOHRU4iByvua",
        "colab_type": "code",
        "outputId": "2c014151-8bba-46e6-a9f2-7c437e4e7c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "best, best_loss = run_style_transfer(content_path, style_path)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best: iteration:  0 loss:  69818950.0   style_loss:  77574080.0   content_loss:  22839.875\n",
            "best: iteration:  2 loss:  42379364.0   style_loss:  47084600.0   content_loss:  32226.734\n",
            "best: iteration:  3 loss:  33203046.0   style_loss:  36888584.0   content_loss:  33216.953\n",
            "best: iteration:  4 loss:  28974022.0   style_loss:  32189352.0   content_loss:  36031.02\n",
            "best: iteration:  5 loss:  25756782.0   style_loss:  28614186.0   content_loss:  40156.277\n",
            "best: iteration:  6 loss:  23703456.0   style_loss:  26332358.0   content_loss:  43324.457\n",
            "best: iteration:  7 loss:  18451060.0   style_loss:  20496308.0   content_loss:  43838.043\n",
            "best: iteration:  8 loss:  14944659.0   style_loss:  16600359.0   content_loss:  43356.516\n",
            "best: iteration:  9 loss:  13165742.0   style_loss:  14623745.0   content_loss:  43722.906\n",
            "best: iteration:  10 loss:  11299999.0   style_loss:  12550522.0   content_loss:  45289.23\n",
            "best: iteration:  11 loss:  10625485.0   style_loss:  11800857.0   content_loss:  47132.957\n",
            "best: iteration:  12 loss:  9497639.0   style_loss:  10547608.0   content_loss:  47916.8\n",
            "best: iteration:  13 loss:  8064542.5   style_loss:  8955279.0   content_loss:  47913.625\n",
            "best: iteration:  14 loss:  7561987.5   style_loss:  8396834.0   content_loss:  48372.39\n",
            "best: iteration:  15 loss:  6904326.5   style_loss:  7665942.0   content_loss:  49777.723\n",
            "best: iteration:  16 loss:  6556498.5   style_loss:  7279266.5   content_loss:  51583.453\n",
            "best: iteration:  17 loss:  6361577.5   style_loss:  7062562.0   content_loss:  52712.895\n",
            "best: iteration:  18 loss:  5692993.5   style_loss:  6319678.5   content_loss:  52828.637\n",
            "best: iteration:  19 loss:  5469285.5   style_loss:  6071117.5   content_loss:  52796.766\n",
            "best: iteration:  20 loss:  5177817.5   style_loss:  5747199.5   content_loss:  53378.99\n",
            "best: iteration:  21 loss:  4905272.5   style_loss:  5444248.5   content_loss:  54484.176\n",
            "best: iteration:  22 loss:  4855808.0   style_loss:  5389195.0   content_loss:  55330.324\n",
            "best: iteration:  23 loss:  4477567.5   style_loss:  4968913.5   content_loss:  55456.95\n",
            "best: iteration:  24 loss:  4301868.0   style_loss:  4773697.0   content_loss:  55412.39\n",
            "best: iteration:  25 loss:  4102744.5   style_loss:  4552401.0   content_loss:  55837.605\n",
            "best: iteration:  26 loss:  3910395.8   style_loss:  4338586.5   content_loss:  56679.26\n",
            "best: iteration:  27 loss:  3829931.0   style_loss:  4249112.0   content_loss:  57308.426\n",
            "best: iteration:  28 loss:  3582696.5   style_loss:  3974395.8   content_loss:  57405.23\n",
            "best: iteration:  29 loss:  3483117.2   style_loss:  3863748.8   content_loss:  57433.8\n",
            "best: iteration:  30 loss:  3321046.2   style_loss:  3683623.5   content_loss:  57851.918\n",
            "best: iteration:  31 loss:  3182130.8   style_loss:  3529200.0   content_loss:  58504.74\n",
            "best: iteration:  32 loss:  3076671.2   style_loss:  3411982.8   content_loss:  58867.062\n",
            "best: iteration:  33 loss:  2915313.2   style_loss:  3232698.2   content_loss:  58849.83\n",
            "best: iteration:  34 loss:  2821599.8   style_loss:  3128563.2   content_loss:  58929.637\n",
            "best: iteration:  35 loss:  2687151.0   style_loss:  2979123.8   content_loss:  59398.0\n",
            "best: iteration:  36 loss:  2614711.8   style_loss:  2898572.8   content_loss:  59961.938\n",
            "best: iteration:  37 loss:  2515269.0   style_loss:  2788057.5   content_loss:  60172.53\n",
            "best: iteration:  38 loss:  2414087.0   style_loss:  2675639.5   content_loss:  60116.957\n",
            "best: iteration:  39 loss:  2333159.8   style_loss:  2585706.8   content_loss:  60237.45\n",
            "best: iteration:  40 loss:  2245354.5   style_loss:  2488103.0   content_loss:  60618.344\n",
            "best: iteration:  41 loss:  2182712.8   style_loss:  2418467.8   content_loss:  60918.25\n",
            "best: iteration:  42 loss:  2095298.1   style_loss:  2321334.8   content_loss:  60969.062\n",
            "best: iteration:  43 loss:  2031784.9   style_loss:  2250755.8   content_loss:  61047.684\n",
            "best: iteration:  44 loss:  1955029.5   style_loss:  2165437.5   content_loss:  61358.105\n",
            "best: iteration:  45 loss:  1896692.9   style_loss:  2100581.2   content_loss:  61697.426\n",
            "best: iteration:  46 loss:  1830616.4   style_loss:  2027150.2   content_loss:  61812.562\n",
            "best: iteration:  47 loss:  1768424.9   style_loss:  1958045.0   content_loss:  61843.902\n",
            "best: iteration:  48 loss:  1709175.8   style_loss:  1892189.9   content_loss:  62049.14\n",
            "best: iteration:  49 loss:  1652349.5   style_loss:  1829016.2   content_loss:  62346.938\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}